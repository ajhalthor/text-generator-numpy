{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to generate sentences using vanilla recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/Ajay/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000 # Cuz it'll take forever otherwise\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "corpora_dir = \"/Users/Ajay/nltk_data/corpora/state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PRESIDENT GEORGE H.W.',\n",
       " \"BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION  January 31, 1990Mr.\",\n",
       " 'President, Mr. Speaker, Members of the United States Congress: I return as a former President of the Senate and a former Member of this great House.',\n",
       " 'And now, as President, it is my privilege to report to you on the state of the Union.',\n",
       " 'Tonight I come not to speak about the state of the Government, not to detail every new initiative we plan for the coming year nor to describe every line in the budget.']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print (\"Reading Data...\")\n",
    "\n",
    "# Read all file paths in corpora directory\n",
    "file_list = []\n",
    "for root, _ , files in os.walk(corpora_dir):  \n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().replace('\\n', '')\n",
    "            sentences.extend(nltk.sent_tokenize(str_form))\n",
    "        except UnicodeDecodeError: \n",
    "            # Some sentences have wierd characters. Ignore them for now\n",
    "            pass\n",
    "\n",
    "# Get all sentences in all files\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentence delimiters\n",
    "sentences = [sentence_start_token + \" \" + x + \" \" + sentence_end_token for x in sentences ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START PRESIDENT GEORGE H.W. SENTENCE_END',\n",
       " \"SENTENCE_START BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION  January 31, 1990Mr. SENTENCE_END\",\n",
       " 'SENTENCE_START President, Mr. Speaker, Members of the United States Congress: I return as a former President of the Senate and a former Member of this great House. SENTENCE_END',\n",
       " 'SENTENCE_START And now, as President, it is my privilege to report to you on the state of the Union. SENTENCE_END',\n",
       " 'SENTENCE_START Tonight I come not to speak about the state of the Government, not to detail every new initiative we plan for the coming year nor to describe every line in the budget. SENTENCE_END']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  18342  unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "\n",
    "print(\"Found \" , len(word_freq.items()) ,\" unique words tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size  8000\n",
      "The least frequent word in our vocabulary is ' three-year ' and appeared  2  times.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1) # Get the most frequent words to construct vocab\n",
    "index_to_word = [x[0] for x in vocab] # Extract word\n",
    "index_to_word.append(unknown_token) # Add an extra token called \"unknown\" for the words in corpora, but not in vocab\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)]) # Create word-index map\n",
    "\n",
    "print (\"Using vocabulary size \", vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '\",vocab[-1][0],\n",
    "       \"' and appeared \",vocab[-1][1],\" times.\")\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training data\n",
    "# Every X represents a word. Every y represents a word that follows it in the sequence\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10th sentence input and expected output to every neuron looks like this\n",
      "\n",
      "[('SENTENCE_START', 'new'), ('new', 'most'), ('most', 'of'), ('of', ','), (',', 'world'), ('world', 'Applause'), ('Applause', 'open'), ('open', 'the'), ('the', ','), (',', 'forward'), ('forward', 'of'), ('of', 'necessary'), ('necessary', 'the'), ('the', 'now'), ('now', '('), ('(', 'our'), ('our', 'democratic'), ('democratic', 'debt'), ('debt', 'the'), ('the', 'system'), ('system', 'economic'), ('economic', 'old'), ('old', 'we'), ('we', 'this'), ('this', 'aid'), ('aid', ','), (',', 'almost'), ('almost', 'of'), ('of', 'our'), ('our', 'you'), ('you', 'build'), ('build', 'in'), ('in', ','), (',', 'by'), ('by', 'that'), ('that', 'remain'), ('remain', '.'), ('.', 'SENTENCE_END')]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[10], y_train[10]\n",
    "# print ( list(zip(x_example, y_example))) \n",
    "print(\"The 10th sentence input and expected output to every neuron looks like this\\n\")\n",
    "print ( list(zip([index_to_word[x] for x in x_example], [index_to_word[y] for y in y_example])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=50, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = model.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print (\"Performing gradient check for parameter \",pname,\" with size \",  np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = model.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print (\"Gradient Check ERROR: parameter= \",pname,\" ix=\",ix)\n",
    "                    print (\"+h Loss: \", gradplus)\n",
    "                    print (\"-h Loss: \", gradminus)\n",
    "                    print (\"Estimated_gradient: \", estimated_gradient)\n",
    "                    print (\"Backpropagation gradient: \", backprop_gradient)\n",
    "                    print (\"Relative Error: \", relative_error)\n",
    "                    return \n",
    "                it.iternext()\n",
    "            print (\"Gradient check for parameter \",pname,\" passed.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 8000)\n",
      "[[0.00012467 0.00012468 0.00012489 ... 0.00012492 0.00012476 0.00012494]\n",
      " [0.00012558 0.00012582 0.00012467 ... 0.00012497 0.00012474 0.00012535]\n",
      " [0.00012561 0.00012434 0.00012451 ... 0.0001249  0.000125   0.00012543]\n",
      " ...\n",
      " [0.00012423 0.00012496 0.0001261  ... 0.00012443 0.00012564 0.00012578]\n",
      " [0.00012596 0.00012544 0.00012486 ... 0.00012441 0.0001262  0.00012466]\n",
      " [0.00012492 0.00012634 0.00012597 ... 0.00012599 0.00012423 0.00012585]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print (o.shape)\n",
    "print (o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output at each timestep is a vector of vocabulary size, that denotes word probabilities. By taking the highest one, we get the next predicted word in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n",
      "[7292 2356 6879 4319 1706 4335 1681 2973  141 6212 5687 5764 4708  141\n",
      " 7814  497 1903 4702 5375 5720 1143  186 4372 3866 4107 1613 2859 1649\n",
      " 5764  820 5791 1511 2286 2137 3265 6224 6477 6806]\n"
     ]
    }
   ],
   "source": [
    "# Proving the predict function works\n",
    "predictions = model.predict(X_train[10])\n",
    "print (predictions.shape)\n",
    "# Gibberish output. We're predicting the next words before even training. LOL\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorably', 'reconstruction', 'characterize', 'armistice', 'elements', 'proportion', 'below', 'phase', 'support', 'torture', 'wholesale', 'wheels', 'announcing', 'support', 'enriching', 'commitment', 'changed', 'Steven', 'accurately', 'emphasized', 'quickly', 'come', 'advise', 'considerable', 'Reverend', 'saved', 'Whatever', 'provisions', 'wheels', 'hopes', 'proceeds', 'size', 'hungry', 'District', 'continuation', 'searching', 'characterized', 'wound']\n"
     ]
    }
   ],
   "source": [
    "print( [index_to_word[w] for w in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions:  8.987196820661973\n",
      "Actual loss:  8.98714522074652\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print (\"Expected Loss for random predictions: \", np.log(vocabulary_size))\n",
    "print (\"Actual loss: \", model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter  U  with size  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ajay/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:111: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter  U  passed.\n",
      "Performing gradient check for parameter  V  with size  1000\n",
      "Gradient check for parameter  V  passed.\n",
      "Performing gradient check for parameter  W  with size  100\n",
      "Gradient check for parameter  W  passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (time,\": Loss after num_examples_seen=\",num_examples_seen,\" epoch=\",epoch,\": \",loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print (\"Setting learning rate to \", learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21 13:29:43 : Loss after num_examples_seen= 0  epoch= 0 :  8.98714522074652\n",
      "2018-11-21 13:30:58 : Loss after num_examples_seen= 1000  epoch= 5 :  6.098045359540115\n",
      "2018-11-21 13:32:11 : Loss after num_examples_seen= 2000  epoch= 10 :  5.724233313979789\n",
      "2018-11-21 13:33:19 : Loss after num_examples_seen= 3000  epoch= 15 :  5.553371330132457\n",
      "2018-11-21 13:34:31 : Loss after num_examples_seen= 4000  epoch= 20 :  5.41661623732687\n",
      "2018-11-21 13:35:40 : Loss after num_examples_seen= 5000  epoch= 25 :  5.318534725306403\n",
      "2018-11-21 13:36:50 : Loss after num_examples_seen= 6000  epoch= 30 :  5.236015662924755\n",
      "2018-11-21 13:38:00 : Loss after num_examples_seen= 7000  epoch= 35 :  5.171466140436469\n",
      "2018-11-21 13:39:15 : Loss after num_examples_seen= 8000  epoch= 40 :  5.106002083005647\n",
      "2018-11-21 13:40:29 : Loss after num_examples_seen= 9000  epoch= 45 :  5.0614697968705435\n",
      "2018-11-21 13:41:42 : Loss after num_examples_seen= 10000  epoch= 50 :  5.0192272785816545\n",
      "2018-11-21 13:43:00 : Loss after num_examples_seen= 11000  epoch= 55 :  4.949533892249479\n",
      "2018-11-21 13:44:20 : Loss after num_examples_seen= 12000  epoch= 60 :  4.903039818800814\n",
      "2018-11-21 13:45:37 : Loss after num_examples_seen= 13000  epoch= 65 :  4.887829714770783\n",
      "2018-11-21 13:47:00 : Loss after num_examples_seen= 14000  epoch= 70 :  4.860000351515549\n",
      "2018-11-21 13:48:19 : Loss after num_examples_seen= 15000  epoch= 75 :  4.776950342888559\n",
      "2018-11-21 13:49:39 : Loss after num_examples_seen= 16000  epoch= 80 :  4.731912235052607\n",
      "2018-11-21 13:51:06 : Loss after num_examples_seen= 17000  epoch= 85 :  4.718620181807108\n",
      "2018-11-21 13:52:27 : Loss after num_examples_seen= 18000  epoch= 90 :  4.831908944431228\n",
      "Setting learning rate to  0.0025\n",
      "2018-11-21 13:53:54 : Loss after num_examples_seen= 19000  epoch= 95 :  4.522437617234355\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "train_with_sgd(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I n't other billion when , you world of , by most `` 's a an of , supply .\n",
      "I with economic taking his peace-loving the year child the enduring was ( switch , value for enemies , American of , also .\n",
      "burden , everything benefiting of , cycle we Our , Congress of more was , ago to , But monopolies to today teachers work , society This in he to pursue to well to without , well of retirement and : fall but our major . national , time every children child we but more this these expenditures our Capitol am in , million fiscal developed employment , live the their between we these hand means number progress in are of United exploring we agricultural will net to national civilian .\n",
      "House specific Applause , system had the freedom the in there economy standstill our Congress will who .\n",
      "new leave at to independent in a until the and -- in create why very living problems bring serve make now , believe of no to 950 must a changes the important you world me , these be or be set to develop to costs on members .\n",
      "I tonight , meet in , world in Applause Nation .\n",
      "security the are should to together , visionary the will We same the we ) long establishment to his it -- in , life must of , legislative .\n",
      "everywhere the wage-price to potential the ? $ most we -- our negotiators expanded and cause to were the , American the , him make will , three been of , school .\n",
      "strategic n't consider to such to Department to our Nation be was .\n",
      "Soviet programs the , rights lands our delinquency have abroad whether our Congress have national ) .\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while (not new_sentence[-1] == word_to_index[sentence_end_token]):\n",
    "        next_word_probs, states = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        \n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        \n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(outfile, model):\n",
    "    U, V, W = model.U, model.V, model.W\n",
    "    np.savez(outfile, U=U, V=V, W=W)\n",
    "    print (\"Saved model parameters to: \", outfile)\n",
    "\n",
    "def load_model(path, model):\n",
    "    npzfile = np.load(path)\n",
    "    U, V, W = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n",
    "    model.hidden_dim = U.shape[0]\n",
    "    model.word_dim = U.shape[1]\n",
    "    model.U = U\n",
    "    model.V = V\n",
    "    model.W = W\n",
    "    print (\"Loaded model parameters from \",path,\". hidden_dim=\",U.shape[0],\" word_dim=\", U.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model parameters to:  learned_model\n"
     ]
    }
   ],
   "source": [
    "save_model(\"learned_model\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model parameters from  learned_model.npz . hidden_dim= 50  word_dim= 8000\n"
     ]
    }
   ],
   "source": [
    "mm = RNN(vocabulary_size)\n",
    "load_model(\"learned_model.npz\", mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-21 14:13:41 : Loss after num_examples_seen= 0  epoch= 0 :  4.484920437574073\n",
      "2018-11-21 14:15:12 : Loss after num_examples_seen= 1000  epoch= 5 :  4.538133982986044\n",
      "Setting learning rate to  0.0025\n",
      "2018-11-21 14:16:34 : Loss after num_examples_seen= 2000  epoch= 10 :  4.467856978524159\n",
      "2018-11-21 14:17:55 : Loss after num_examples_seen= 3000  epoch= 15 :  4.439540265729607\n",
      "2018-11-21 14:19:19 : Loss after num_examples_seen= 4000  epoch= 20 :  4.377615473275601\n",
      "2018-11-21 14:20:43 : Loss after num_examples_seen= 5000  epoch= 25 :  4.341018487704967\n",
      "2018-11-21 14:22:04 : Loss after num_examples_seen= 6000  epoch= 30 :  4.342171445984887\n",
      "Setting learning rate to  0.00125\n",
      "2018-11-21 14:23:28 : Loss after num_examples_seen= 7000  epoch= 35 :  4.192869267977804\n",
      "2018-11-21 14:24:49 : Loss after num_examples_seen= 8000  epoch= 40 :  4.196551819799632\n",
      "Setting learning rate to  0.000625\n",
      "2018-11-21 14:26:13 : Loss after num_examples_seen= 9000  epoch= 45 :  4.090430858002756\n",
      "2018-11-21 14:27:34 : Loss after num_examples_seen= 10000  epoch= 50 :  4.053730662129732\n",
      "2018-11-21 14:28:56 : Loss after num_examples_seen= 11000  epoch= 55 :  4.0149648529469255\n",
      "2018-11-21 14:30:19 : Loss after num_examples_seen= 12000  epoch= 60 :  3.9886808322730873\n",
      "2018-11-21 14:31:41 : Loss after num_examples_seen= 13000  epoch= 65 :  3.9797364581067893\n",
      "2018-11-21 14:33:03 : Loss after num_examples_seen= 14000  epoch= 70 :  3.962801546898589\n",
      "2018-11-21 14:34:25 : Loss after num_examples_seen= 15000  epoch= 75 :  3.95148927296913\n",
      "2018-11-21 14:35:49 : Loss after num_examples_seen= 16000  epoch= 80 :  3.9385107627011817\n",
      "2018-11-21 14:37:11 : Loss after num_examples_seen= 17000  epoch= 85 :  3.942303394811109\n",
      "Setting learning rate to  0.0003125\n",
      "2018-11-21 14:38:35 : Loss after num_examples_seen= 18000  epoch= 90 :  3.8929458590103376\n",
      "2018-11-21 14:39:57 : Loss after num_examples_seen= 19000  epoch= 95 :  3.9022707339773315\n",
      "Setting learning rate to  0.00015625\n"
     ]
    }
   ],
   "source": [
    "train_with_sgd(mm, X_train, y_train) # Continue training where we left off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
